{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dVaxBgoktdw",
        "outputId": "fece6e95-0212-4dfb-ef19-c5c0291ea975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.7.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic code for llm calling\n"
      ],
      "metadata": {
        "id": "wr53HOwloGTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install huggingface_hub if not already installed\n",
        "# !pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# Replace with your actual token\n",
        "\n",
        "\n",
        "# You can choose models like gpt2, mistralai/Mistral-7B-Instruct-v0.1, etc.\n",
        "model = \"mistralai/Devstral-Small-2507\"\n",
        "\n",
        "# Create the inference client\n",
        "client = InferenceClient(model=model, token=hf_token)\n",
        "\n",
        "# Take user input\n",
        "user_question = input(\"Ask your question: \")\n",
        "\n",
        "# Format the prompt (instruction-style)\n",
        "prompt = f\"[INST] {user_question} [/INST]\"\n",
        "\n",
        "# Generate response\n",
        "response = client.text_generation(\n",
        "    prompt=prompt,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Print the model's response\n",
        "print(\"\\nðŸ¤– Assistant:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF8Eo7f5m5Ri",
        "outputId": "cae71217-d668-479a-970a-b86bd99c71cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask your question: how are you ?\n",
            "\n",
            "ðŸ¤– Assistant: I'm an artificial intelligence and don't have feelings, but thank you for asking! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual token\n",
        "hf_token = \"\"\n",
        "\n",
        "# You can choose models like gpt2, mistralai/Mistral-7B-Instruct-v0.1, etc.\n",
        "model = \"mistralai/Devstral-Small-2507\""
      ],
      "metadata": {
        "id": "msV13IDSn0nn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "id": "2ZkldT59ntB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With UI"
      ],
      "metadata": {
        "id": "1iup3HHRoL8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "\n",
        "# Set up the client\n",
        "client = InferenceClient(model=model, token=hf_token)\n",
        "\n",
        "# Define the chat function with memory\n",
        "def chat_with_memory(message, history):\n",
        "    \"\"\"\n",
        "    message: current user input (str)\n",
        "    history: list of previous (user, assistant) tuples\n",
        "    \"\"\"\n",
        "    # Build prompt from chat history\n",
        "    prompt_parts = []\n",
        "    for user_msg, bot_msg in history:\n",
        "        prompt_parts.append(f\"[INST] {user_msg} [/INST] {bot_msg}\")\n",
        "    prompt_parts.append(f\"[INST] {message} [/INST]\")\n",
        "\n",
        "    # Join all into final prompt\n",
        "    full_prompt = \"\\n\".join(prompt_parts)\n",
        "\n",
        "    # Query the model\n",
        "    response = client.text_generation(\n",
        "        prompt=full_prompt,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Return only the assistant's response, history is auto-updated by Gradio\n",
        "    return response\n",
        "\n",
        "# Create the chat interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_memory,\n",
        "    title=\"ðŸ¤– Chat with Mistral LLM\",\n",
        "    description=\"Memory-aware assistant powered by Hugging Face's mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    theme=\"soft\",\n",
        ")\n",
        "\n",
        "# Launch the UI\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "489izCz5nxWP",
        "outputId": "dbcfd9d5-fad0-472d-811d-abf5cfdf5e13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://69066bbf8e0f1a054f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://69066bbf8e0f1a054f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}